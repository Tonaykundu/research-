# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19pNUWdfhsah1-0rRSBo4XK31kxOmyDbM
"""

! nvidia-smi

from google.colab import drive
drive.mount('/content/drive')

import cv2
import numpy as np
import glob
import os

pip install split-folders

import splitfolders
import os

# **Please update this to the path of your input image folder**
input_folder="/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g"

# **Please update this to the desired path for the output directory**
train_test_val="/content/drive/MyDrive/222-15-6386  Tonay kundu/output traintest val"

splitfolders.ratio(input_folder, train_test_val, seed=42, ratio=(0.7,0.2,0.1)) ### train 50%, val 50%, test 0%

import tensorflow as tf
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt

####### transfer learning model

################# import various transfer elarning models ###################

#from tensorflow.keras.applications.inception_v3 import InceptionV3
#from tensorflow.keras.applications.vgg16 import VGG16
#from tensorflow.keras.applications.vgg19 import VGG19
#from tensorflow.keras.applications import Xception
from tensorflow.keras.applications import Xception
#############################################################################

from tensorflow.keras.layers import Input, Lambda, Dense, Flatten
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
import numpy as np
from glob import glob

import keras.backend as K
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, CSVLogger
from tensorflow.keras.applications.vgg16 import VGG16
import tensorflow as tf

model = models.Sequential()

model.add(layers.Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))

model.add(layers.Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(layers.MaxPooling2D((2, 2)))  # একবার আরো MaxPooling দেয়া হলো

model.add(layers.Flatten())


model.add(layers.Dense(128, activation='softmax'))
model.add(layers.Dense(2))

adam = tf.keras.optimizers.Adam(learning_rate=0.01)

model.compile(
  loss='categorical_crossentropy', ### for multi class calsification
  #loss='binary_crossentropy', ### for binary calsification
  optimizer=adam,
  metrics=['accuracy']
)

#########################################################################

model.optimizer.get_config()
print("\n\n")
model.summary()

### image datagenerator
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/train',
                                                 target_size = (224,224),
                                                 batch_size = 8,
                                                 class_mode = 'categorical')

val_set = val_datagen.flow_from_directory( '/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/validation',
                                            target_size = (224,224),
                                            batch_size = 8,
                                            class_mode = 'categorical')

test_datagen = ImageDataGenerator(rescale = 1./255)

test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/test',
                                            target_size = (224,224),
                                            batch_size = 8,
                                            class_mode = 'categorical')

from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger

# Define the correct filepath ending in `.weights.h5`
filepath = "/content/drive/MyDrive/222-15-6386  Tonay kundu/output traintest val/already train model.weights.h5"

# Define the ModelCheckpoint
checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,
                               save_weights_only=True, save_best_only=True, mode='max')

# Define the CSVLogger
log_csv = CSVLogger('/content/drive/MyDrive/222-15-6386  Tonay kundu/cnn.csv', separator=',', append=False)

# List of callbacks
callbacks_list = [checkpoint1, log_csv]

# Fit the model
r = model.fit(
    training_set,
    epochs=5,
    validation_data=val_set,
    steps_per_epoch=len(training_set),
    validation_steps=len(val_set),
    callbacks=callbacks_list # Add the callbacks here
)

import pandas as pd
import matplotlib.pyplot as plt

# Load data from CSV file
data = pd.read_csv('/content/drive/MyDrive/222-15-6386  Tonay kundu/cnn.csv')

# Plot accuracy
plt.plot(data['epoch'], data['accuracy'], label='Training Accuracy')
plt.plot(data['epoch'], data['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Curve')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(data['epoch'], data['loss'], label='Training Loss')
plt.plot(data['epoch'], data['val_loss'], label='Validation Loss')
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Load weights
model.load_weights('/content/drive/MyDrive/222-15-6386  Tonay kundu/output traintest val/already train model.weights.h5')  ### Load weights from the .weights.h5 file

# Evaluate the model
preds = model.evaluate(test_set)

# Print results
print("Loss = " + str(preds[0]))
print("Test Accuracy = " + str(preds[1]))

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions using the trained model
y_pred = model.predict(test_set)  # Use model.predict instead of model.predict_generator
y_true = test_set.labels  # Ground truth labels

# Generate a confusion matrix
confusion_mat = confusion_matrix(y_true, np.argmax(y_pred, axis=1))

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=test_set.class_indices.keys(), yticklabels=test_set.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# You can also generate a classification report for more detailed metrics
class_report = classification_report(y_true, np.argmax(y_pred, axis=1), target_names=test_set.class_indices.keys())
print(class_report)

IMAGE_SIZE = [224,224]
CLASS=2
######


#mod =  tf.keras.applications.VGG19(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
#mod = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
#mod = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
#mod = DenseNet201(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
mod = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
#mod = Xception(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
#mod = InceptionV3(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)

for layer in mod.layers:
    layer.trainable = False

x = Flatten()(mod.output)

prediction = Dense(CLASS, activation='elu')(x)

model = Model(inputs=mod.input, outputs=prediction)

############################ optimizer and learning rate ##################

adam = tf.keras.optimizers.Nadam(learning_rate=0.01)

model.compile(
  loss='categorical_crossentropy', ### for multi class calsification
  #loss='binary_crossentropy', ### for binary calsification
  optimizer=adam,
  metrics=['accuracy']
)

#########################################################################


model.optimizer.get_config()
print("\n\n")
model.summary()

### image datagenerator

train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=20,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/train',
                                                 target_size = (224,224),
                                                 batch_size = 8,
                                                 class_mode = 'categorical')

val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/validation',
                                            target_size = (224,224),
                                            batch_size = 8,
                                            class_mode = 'categorical')

test_datagen = ImageDataGenerator(rescale = 1./255)

test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/222-15-6386  Tonay kundu/imf=g/Breast Cancer/dataset/test',
                                            target_size = (224,224),
                                            batch_size = 8,
                                            class_mode = 'categorical')

from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger

# Define the correct filepath ending in `.weights.h5`
filepath = "/content/VGG16.weights.h5"

# Define the ModelCheckpoint
checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,
                               save_weights_only=True, save_best_only=True, mode='max')

# Define the CSVLogger
log_csv = CSVLogger('/content/VGG16.csv', separator=',', append=False)

# List of callbacks
callbacks_list = [checkpoint1, log_csv]

# Fit the model
r = model.fit(
    training_set,
    epochs=10,
    validation_data=val_set,
    steps_per_epoch=len(training_set),
    validation_steps=len(val_set),
)

import pandas as pd
import matplotlib.pyplot as plt

# Load data from CSV file
data = pd.read_csv('/content/drive/MyDrive/222-15-6386  Tonay kundu/cnn.csv')

# Plot accuracy
plt.plot(data['epoch'], data['accuracy'], label='Training Accuracy')
plt.plot(data['epoch'], data['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Curve')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# Plot loss
plt.plot(data['epoch'], data['loss'], label='Training Loss')
plt.plot(data['epoch'], data['val_loss'], label='Validation Loss')
plt.title('Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

# Load weights
model.load_weights('/content/drive/MyDrive/newmodel.weights.h5')  ### Load weights from the .weights.h5 file

# Evaluate the model
preds = model.evaluate(test_set)

# Print results
print("Loss = " + str(preds[0]))
print("Test Accuracy = " + str(preds[1]))

import numpy as np
from sklearn.metrics import confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns

# Make predictions using the trained model
y_pred = model.predict(test_set)  # Use model.predict instead of model.predict_generator
y_true = test_set.labels  # Ground truth labels

# Generate a confusion matrix
confusion_mat = confusion_matrix(y_true, np.argmax(y_pred, axis=1))

# Visualize the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion_mat, annot=True, fmt="d", cmap="Blues", xticklabels=test_set.class_indices.keys(), yticklabels=test_set.class_indices.keys())
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# You can also generate a classification report for more detailed metrics
class_report = classification_report(y_true, np.argmax(y_pred, axis=1), target_names=test_set.class_indices.keys())
print(class_report)